{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 사용할 패키지 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build an autoencoder-based anomaly detection model, we'll use Keras, an open-source deep learning library written in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Pandas의 read_excel 함수를 사용하면 xlsx 형식의 파일을 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_excel('Train.xlsx', index_col=0)\n",
    "valid_data = pd.read_excel('Test.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 데이터의 정보가 1번 Index부터 시작하므로 전처리를 해줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns = train_data.iloc[0, :]\n",
    "train_data = train_data.iloc[1:, ].reset_index(drop = True)\n",
    "\n",
    "valid_data.columns = valid_data.iloc[0, :]\n",
    "valid_data = valid_data.iloc[1:, 1:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning 모델은 X 변수들을 정규화해야 한다.  \n",
    "이 때, Scikit-learn 패키지의 MinMaxScalar를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define AutoEncoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Shape와 Encoding Dimension을 정해주면, Keras 패키지에 내장되어 있는 함수들로 Auto Encoder 모델을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_model(encoding_dim):\n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=52)\n",
    "    encoded = layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = layers.Dense(52, activation='sigmoid')(encoded)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    autoencoder = models.Model(input_layer, decoded)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    \n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyper-parameter Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn 패키지의 GridSearchCV 함수를 사용하면, Hyper-parameter tuning을 수행할 수 있다.  \n",
    "params (Dictionary)에 파라미터 목록을 추가하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for grid search\n",
    "param_grid = {'encoding_dim': [10, 20, 30, 40]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the autoencoder using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26665/299675781.py:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  autoencoder = tf.keras.wrappers.scikit_learn.KerasRegressor(\n"
     ]
    }
   ],
   "source": [
    "input_shape = train_data_scaled.shape[1]\n",
    "autoencoder = tf.keras.wrappers.scikit_learn.KerasRegressor(\n",
    "    build_fn=autoencoder_model, epochs=50, batch_size=32, verbose=0)\n",
    "grid_search = GridSearchCV(autoencoder, param_grid=param_grid, cv=3)\n",
    "grid_result = grid_search.fit(train_data_scaled, train_data_scaled)\n",
    "best_encoding_dim = grid_result.best_params_['encoding_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'encoding_dim': 40}\n"
     ]
    }
   ],
   "source": [
    "# Fit final model with best hyperparameters\n",
    "best_params = grid_result.best_params_\n",
    "print('Best hyperparameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f0c61fd00>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = autoencoder_model(best_encoding_dim)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder.fit(train_data_scaled, train_data_scaled, epochs=50, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터셋 정규화 수행\n",
    "valid_data_scaled = scaler.transform(valid_data)\n",
    "\n",
    "# 학습 된 모델로 예측 값 생성\n",
    "valid_data_pred = autoencoder.predict(valid_data_scaled)\n",
    "\n",
    "# MSE를 활용한 Anomaly Score 생성\n",
    "valid_data_mse = np.mean(np.power(valid_data_scaled - valid_data_pred, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data['Process variables'] = valid_data_mse\n",
    "valid_data['Process variables'] = np.where(valid_data['Process variables'] > valid_data['Process variables'].mean(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.to_excel('AnomalyDetectionResults.xlsx')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2fa2dea287701ca5566d6aec2370305f5b01ed80b4ee77198dcb04a55d6d85b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf280')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
